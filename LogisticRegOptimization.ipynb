{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQC5HB7ywu4lpN6oj+0OOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdoAlkayal/Machine_Learning/blob/main/LogisticRegOptimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ggrHtEj70h6k"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score , confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rice_dataset_raw = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/Rice_Cammeo_Osmancik.csv\")\n",
        "x=rice_dataset_raw.drop('Class',axis=1)\n",
        "y=rice_dataset_raw['Class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "print(rice_dataset_raw.describe())\n",
        "print(\"\\nData Shapes:\\n\" + \"-\" * 40)\n",
        "print(f\"{'X_train shape':<15}: {X_train.shape}\")\n",
        "print(f\"{'X_test shape' :<15}: {X_test.shape}\")\n",
        "print(f\"{'y_train shape':<15}: {y_train.shape}\")\n",
        "print(f\"{'y_test shape' :<15}: {y_test.shape}\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "print(\"\\nLabel Encoding Map:\\n\" + \"-\" * 40)\n",
        "for class_name, class_id in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):\n",
        "    print(f\"{class_name:<10} → {class_id}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2ZjDikh6dIh",
        "outputId": "31f24b04-ad30-4f4e-d486-7203507fd182"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Area    Perimeter  Major_Axis_Length  Minor_Axis_Length  \\\n",
            "count   3810.000000  3810.000000        3810.000000        3810.000000   \n",
            "mean   12667.727559   454.239180         188.776222          86.313750   \n",
            "std     1732.367706    35.597081          17.448679           5.729817   \n",
            "min     7551.000000   359.100006         145.264465          59.532406   \n",
            "25%    11370.500000   426.144753         174.353855          82.731695   \n",
            "50%    12421.500000   448.852493         185.810059          86.434647   \n",
            "75%    13950.000000   483.683746         203.550438          90.143677   \n",
            "max    18913.000000   548.445984         239.010498         107.542450   \n",
            "\n",
            "       Eccentricity   Convex_Area       Extent  \n",
            "count   3810.000000   3810.000000  3810.000000  \n",
            "mean       0.886871  12952.496850     0.661934  \n",
            "std        0.020818   1776.972042     0.077239  \n",
            "min        0.777233   7723.000000     0.497413  \n",
            "25%        0.872402  11626.250000     0.598862  \n",
            "50%        0.889050  12706.500000     0.645361  \n",
            "75%        0.902588  14284.000000     0.726562  \n",
            "max        0.948007  19099.000000     0.861050  \n",
            "\n",
            "Data Shapes:\n",
            "----------------------------------------\n",
            "X_train shape  : (3048, 7)\n",
            "X_test shape   : (762, 7)\n",
            "y_train shape  : (3048,)\n",
            "y_test shape   : (762,)\n",
            "\n",
            "Label Encoding Map:\n",
            "----------------------------------------\n",
            "Cammeo     → 0\n",
            "Osmancik   → 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "scaler=StandardScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "8iYHfeA2_Wj0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "source": [
        "class Batch_Gradient_Descent:\n",
        "  def __init__(self, learning_rate=0.001, n_iters=1000):\n",
        "    self.lr = learning_rate\n",
        "    self.n_iters = n_iters\n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "  def loss(self, y, y_pred):\n",
        "    return (-y*np.log(y_pred) - (1-y)*np.log(1-y_pred)).mean()\n",
        "  def fit(self, X, y):\n",
        "    self.weights =np.zeros(X.shape[1])\n",
        "    self.bias = 0\n",
        "    self.losses = []\n",
        "    for i in range(self.n_iters):\n",
        "      z_pred = np.dot(X, self.weights) + self.bias\n",
        "      y_pred = self.sigmoid(z_pred)\n",
        "      dw = (1/X.shape[0])*np.dot(X.T, (y_pred-y))\n",
        "      db = (1/X.shape[0])*np.sum(y_pred-y)\n",
        "      self.weights -= self.lr*dw\n",
        "      self.bias -= self.lr*db\n",
        "      self.losses.append(self.loss(y, y_pred))\n",
        "      print(f\"epoch:{i+1}/{self.n_iters}, loss:{self.losses[-1]}\")\n",
        "  def predict(self, X):\n",
        "    z_pred = np.dot(X, self.weights) + self.bias\n",
        "    y_pred = self.sigmoid(z_pred)\n",
        "    y_pred = np.where(y_pred>0.5, 1, 0)\n",
        "    return y_pred"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IP1dmpnZBYhp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mini_batch_gradient_descent:\n",
        "  def __init__(self, learning_rate=0.001, n_iters=1000, batch_size=32):\n",
        "    self.lr = learning_rate\n",
        "    self.n_iters = n_iters\n",
        "    self.batch_size = batch_size\n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "  def loss(self, y, y_pred):\n",
        "    return (-y*np.log(y_pred) - (1-y)*np.log(1-y_pred)).mean()\n",
        "  def fit(self, X, y):\n",
        "    self.weights =np.zeros(X.shape[1])\n",
        "    self.bias = 0\n",
        "    self.losses = []\n",
        "    n_samples=X.shape[0]\n",
        "    for i in range(self.n_iters):\n",
        "     indices = np.arange(n_samples)\n",
        "     np.random.shuffle(indices)\n",
        "     X = X[indices]\n",
        "     y = y[indices]\n",
        "     for j in range(0, n_samples, self.batch_size):\n",
        "       X_batch = X[j:j+self.batch_size]\n",
        "       y_batch = y[j:j+self.batch_size]\n",
        "       z_pred = np.dot(X_batch, self.weights) + self.bias\n",
        "       y_pred = self.sigmoid(z_pred)\n",
        "       dw = (1/self.batch_size)*np.dot(X_batch.T, (y_pred-y_batch))\n",
        "       db = (1/self.batch_size)*np.sum(y_pred-y_batch)\n",
        "       self.weights -= self.lr*dw\n",
        "       self.bias -= self.lr*db\n",
        "       self.losses.append(self.loss(y_batch, y_pred))\n",
        "     print(f\"epoch:{i+1}/{self.n_iters}, loss:{self.losses[-1]}\")\n",
        "  def predict(self, X):\n",
        "    y_pred = np.dot(X, self.weights) + self.bias\n",
        "    y_pred = self.sigmoid(y_pred)\n",
        "    y_pred = np.where(y_pred>0.5, 1, 0)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "Et1xhEbCL-J7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stochastic_gradient_descent:\n",
        "  def __init__(self, learning_rate=0.001, n_iters=1000):\n",
        "    self.lr = learning_rate\n",
        "    self.n_iters = n_iters\n",
        "  def sigmoid(self, x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "  def loss(self, y, y_pred):\n",
        "    return (-y*np.log(y_pred) - (1-y)*np.log(1-y_pred)).mean()\n",
        "  def fit(self, X, y):\n",
        "    self.weights =np.zeros(X.shape[1])\n",
        "    self.bias = 0\n",
        "    self.losses = []\n",
        "    n_samples=X.shape[0]\n",
        "    for i in range(self.n_iters):\n",
        "     indices = np.arange(n_samples)\n",
        "     np.random.shuffle(indices)\n",
        "     X_shuffled = X[indices]\n",
        "     y_shuffled= y[indices]\n",
        "     for j in range(n_samples):\n",
        "       X_j = X_shuffled[j].reshape(1, -1)\n",
        "       y_j = y_shuffled[j]\n",
        "       z_pred = np.dot(X_j, self.weights) + self.bias\n",
        "       y_pred = self.sigmoid(z_pred)\n",
        "       dw = np.dot(X_j.T, (y_pred-y_j))\n",
        "       db = np.sum(y_pred-y_j)\n",
        "       self.weights -= self.lr*dw\n",
        "       self.bias -= self.lr*db\n",
        "       self.losses.append(self.loss(y_j, y_pred))\n",
        "     print(f\"epoch:{i+1}/{self.n_iters}, loss:{self.losses[-1]}\")\n",
        "  def predict(self, X):\n",
        "    z_pred = np.dot(X, self.weights) + self.bias\n",
        "    y_pred = self.sigmoid(z_pred)\n",
        "    y_pred = np.where(y_pred>0.5, 1, 0)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "j6e4IQF1MB18"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Batch_Gradient_Descent(learning_rate=0.01, n_iters=10)\n",
        "model1.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\\n\" + \"-\" * 30)\n",
        "print(cm)\n",
        "\n",
        "# Evaluation Metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\\n\" + \"-\" * 30)\n",
        "print(f\"{'Accuracy' :<10}: {acc:.4f}\")\n",
        "print(f\"{'Precision':<10}: {prec:.4f}\")\n",
        "print(f\"{'Recall'   :<10}: {rec:.4f}\")\n",
        "print(f\"{'F1 Score' :<10}: {f1:.4f}\")\n",
        "results['BGD'] = acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h70lpF9MHEg",
        "outputId": "3e4c2f12-49f7-4154-8df9-7bb7ea3e3b7a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1/10, loss:0.6931471805599454\n",
            "epoch:2/10, loss:0.6859784498399223\n",
            "epoch:3/10, loss:0.6789686275814459\n",
            "epoch:4/10, loss:0.6721140603671435\n",
            "epoch:5/10, loss:0.6654111313974577\n",
            "epoch:6/10, loss:0.6588562648063736\n",
            "epoch:7/10, loss:0.6524459295717169\n",
            "epoch:8/10, loss:0.6461766430308732\n",
            "epoch:9/10, loss:0.6400449740147822\n",
            "epoch:10/10, loss:0.634047545614724\n",
            "\n",
            "Confusion Matrix:\n",
            "------------------------------\n",
            "[[323  27]\n",
            " [ 36 376]]\n",
            "\n",
            "Evaluation Metrics:\n",
            "------------------------------\n",
            "Accuracy  : 0.9173\n",
            "Precision : 0.9330\n",
            "Recall    : 0.9126\n",
            "F1 Score  : 0.9227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model2 = Mini_batch_gradient_descent(learning_rate=0.01, n_iters=10, batch_size=2)\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\\n\" + \"-\" * 30)\n",
        "print(cm)\n",
        "\n",
        "# Evaluation Metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\\n\" + \"-\" * 30)\n",
        "print(f\"{'Accuracy' :<10}: {acc:.4f}\")\n",
        "print(f\"{'Precision':<10}: {prec:.4f}\")\n",
        "print(f\"{'Recall'   :<10}: {rec:.4f}\")\n",
        "print(f\"{'F1 Score' :<10}: {f1:.4f}\")\n",
        "results['MBGD'] = acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKP268gBMMxc",
        "outputId": "b414ab83-9cb6-43ad-f371-a2045c58f454"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1/10, loss:0.05672396999092577\n",
            "epoch:2/10, loss:0.007518752924967652\n",
            "epoch:3/10, loss:0.004389158741569514\n",
            "epoch:4/10, loss:0.2580398497857103\n",
            "epoch:5/10, loss:0.005426352708584119\n",
            "epoch:6/10, loss:0.0037513371270482972\n",
            "epoch:7/10, loss:0.05628114881281944\n",
            "epoch:8/10, loss:0.11889952592491723\n",
            "epoch:9/10, loss:0.23100342951649822\n",
            "epoch:10/10, loss:0.380133758471084\n",
            "\n",
            "Confusion Matrix:\n",
            "------------------------------\n",
            "[[324  26]\n",
            " [ 25 387]]\n",
            "\n",
            "Evaluation Metrics:\n",
            "------------------------------\n",
            "Accuracy  : 0.9331\n",
            "Precision : 0.9370\n",
            "Recall    : 0.9393\n",
            "F1 Score  : 0.9382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model3= Stochastic_gradient_descent(learning_rate=0.01, n_iters=10)\n",
        "model3.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model3.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\\n\" + \"-\" * 30)\n",
        "print(cm)\n",
        "\n",
        "# Evaluation Metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\\n\" + \"-\" * 30)\n",
        "print(f\"{'Accuracy' :<10}: {acc:.4f}\")\n",
        "print(f\"{'Precision':<10}: {prec:.4f}\")\n",
        "print(f\"{'Recall'   :<10}: {rec:.4f}\")\n",
        "print(f\"{'F1 Score' :<10}: {f1:.4f}\")\n",
        "results['SGD'] = acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYEKfzPMMQ_G",
        "outputId": "b8fc70e9-0485-4c40-e8f3-e5c22a27fccb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1/10, loss:0.11076787509088494\n",
            "epoch:2/10, loss:0.27607446467164043\n",
            "epoch:3/10, loss:0.05722413745548109\n",
            "epoch:4/10, loss:0.0029558230420120776\n",
            "epoch:5/10, loss:0.009500469276133114\n",
            "epoch:6/10, loss:0.019303892931476614\n",
            "epoch:7/10, loss:0.0005870217083080479\n",
            "epoch:8/10, loss:0.0009622693646595855\n",
            "epoch:9/10, loss:0.0005902875666258848\n",
            "epoch:10/10, loss:0.01191478657622827\n",
            "\n",
            "Confusion Matrix:\n",
            "------------------------------\n",
            "[[321  29]\n",
            " [ 25 387]]\n",
            "\n",
            "Evaluation Metrics:\n",
            "------------------------------\n",
            "Accuracy  : 0.9291\n",
            "Precision : 0.9303\n",
            "Recall    : 0.9393\n",
            "F1 Score  : 0.9348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cOr8CTw66t-V"
      }
    }
  ]
}